{"updateTime":"2024-10-09 02:26:29","data":[{"username":"karpathy","name":"Andrej","type":"user","url":"https://github.com/karpathy","avatar":"https://avatars.githubusercontent.com/u/241138?s=96&v=4","repo":{"name":"llm.c","description":"LLM training in simple, raw C/CUDA","url":"https://github.com/karpathy/llm.c"}},{"username":"DefTruth","name":"DefTruth","type":"user","url":"https://github.com/DefTruth","avatar":"https://avatars.githubusercontent.com/u/31974251?s=96&v=4","repo":{"name":"CUDA-Learn-Notes","description":"ðŸŽ‰ Modern CUDA Learn Notes with PyTorch: fp32, fp16, bf16, fp8/int8, flash_attn, sgemm, sgemv, warp/block reduce, dot, elementwise, softmaâ€¦","url":"https://github.com/DefTruth/CUDA-Learn-Notes"}},{"username":"siboehm","name":"Simon Boehm","type":"user","url":"https://github.com/siboehm","avatar":"https://avatars.githubusercontent.com/u/14908678?s=96&v=4","repo":{"name":"SGEMM_CUDA","description":"Fast CUDA matrix multiplication from scratch","url":"https://github.com/siboehm/SGEMM_CUDA"}}]}