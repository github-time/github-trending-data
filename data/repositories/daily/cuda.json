{"updateTime":"2023-01-21 02:21:14","data":[{"full_name":"NVlabs/instant-ngp","description":"Instant neural graphics primitives: lightning fast NeRF and more","currentPeriodStars":17,"language":"Cuda","languageColor":"#3A4E3A","stargazers_count":10827,"forks_count":1300,"owner":{"avatar_url":"https://avatars.githubusercontent.com/u/4923655?s=40&v=4","login":"Tom94"},"buildBy":[{"avatar_url":"https://avatars.githubusercontent.com/u/4923655?s=40&v=4","login":"Tom94"},{"avatar_url":"https://avatars.githubusercontent.com/u/7601391?s=40&v=4","login":"FlorisE"},{"avatar_url":"https://avatars.githubusercontent.com/u/29726242?s=40&v=4","login":"jc211"},{"avatar_url":"https://avatars.githubusercontent.com/u/7057863?s=40&v=4","login":"yenchenlin"},{"avatar_url":"https://avatars.githubusercontent.com/u/3280839?s=40&v=4","login":"JamesPerlman"}]},{"full_name":"MARD1NO/FxxkCUDA","description":"","currentPeriodStars":0,"stargazers_count":0,"forks_count":0,"owner":{},"buildBy":[]},{"full_name":"NVIDIA/TransformerEngine","description":"A library for accelerating Transformer models on NVIDIA GPUs, including using 8-bit floating point (FP8) precision on Hopper GPUs, to provide better performance with lower memory utilization in both training and inference.","currentPeriodStars":0,"language":"Cuda","languageColor":"#3A4E3A","stargazers_count":357,"forks_count":39,"owner":{"avatar_url":"https://avatars.githubusercontent.com/u/36168853?s=40&v=4","login":"ksivaman"},"buildBy":[{"avatar_url":"https://avatars.githubusercontent.com/u/36168853?s=40&v=4","login":"ksivaman"},{"avatar_url":"https://avatars.githubusercontent.com/u/8398980?s=40&v=4","login":"ptrendx"},{"avatar_url":"https://avatars.githubusercontent.com/u/4406448?s=40&v=4","login":"timmoon10"},{"avatar_url":"https://avatars.githubusercontent.com/u/14275056?s=40&v=4","login":"zlsh80826"},{"avatar_url":"https://avatars.githubusercontent.com/u/883319?s=40&v=4","login":"seanprime7"}]}]}