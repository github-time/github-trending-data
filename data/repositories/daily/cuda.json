{"updateTime":"2025-07-14 02:30:07","data":[{"full_name":"","description":"NVIDIA cuOpt is an open-source GPU-accelerated optimization engine delivering near real-time solutions for complex decision-making challenges.","currentPeriodStars":3,"language":"Cuda","languageColor":"#3A4E3A","stargazers_count":277,"forks_count":0,"owner":{"avatar_url":"https://avatars.githubusercontent.com/u/42624703?s=40&v=4","login":"rgsl888prabhu"},"buildBy":[{"avatar_url":"https://avatars.githubusercontent.com/u/42624703?s=40&v=4","login":"rgsl888prabhu"},{"avatar_url":"https://avatars.githubusercontent.com/u/184431765?s=40&v=4","login":"chris-maes"},{"avatar_url":"https://avatars.githubusercontent.com/u/6844564?s=40&v=4","login":"rg20"},{"avatar_url":"https://avatars.githubusercontent.com/u/31096601?s=40&v=4","login":"Kh4ster"},{"avatar_url":"https://avatars.githubusercontent.com/u/160623740?s=40&v=4","login":"aliceb-nv"}]},{"full_name":"","description":"A massively parallel, optimal functional runtime in Rust","currentPeriodStars":4,"language":"Cuda","languageColor":"#3A4E3A","stargazers_count":11062,"forks_count":0,"owner":{"avatar_url":"https://avatars.githubusercontent.com/u/13090166?s=40&v=4","login":"VictorTaelin"},"buildBy":[{"avatar_url":"https://avatars.githubusercontent.com/u/13090166?s=40&v=4","login":"VictorTaelin"},{"avatar_url":"https://avatars.githubusercontent.com/u/5505315?s=40&v=4","login":"enricozb"},{"avatar_url":"https://avatars.githubusercontent.com/u/53550620?s=40&v=4","login":"kings177"},{"avatar_url":"https://avatars.githubusercontent.com/u/30930225?s=40&v=4","login":"edusporto"},{"avatar_url":"https://avatars.githubusercontent.com/u/44031566?s=40&v=4","login":"tjjfvi"}]},{"full_name":"","description":"CUDA accelerated rasterization of gaussian splatting","currentPeriodStars":2,"language":"Cuda","languageColor":"#3A4E3A","stargazers_count":3294,"forks_count":0,"owner":{"avatar_url":"https://avatars.githubusercontent.com/u/10151885?s=40&v=4","login":"liruilong940607"},"buildBy":[{"avatar_url":"https://avatars.githubusercontent.com/u/10151885?s=40&v=4","login":"liruilong940607"},{"avatar_url":"https://avatars.githubusercontent.com/u/5694811?s=40&v=4","login":"vye16"},{"avatar_url":"https://avatars.githubusercontent.com/u/30566358?s=40&v=4","login":"maturk"},{"avatar_url":"https://avatars.githubusercontent.com/u/15806475?s=40&v=4","login":"kerrj"},{"avatar_url":"https://avatars.githubusercontent.com/u/6992947?s=40&v=4","login":"brentyi"}]},{"full_name":"","description":"FlashInfer: Kernel Library for LLM Serving","currentPeriodStars":6,"language":"Cuda","languageColor":"#3A4E3A","stargazers_count":3354,"forks_count":0,"owner":{"avatar_url":"https://avatars.githubusercontent.com/u/11773619?s=40&v=4","login":"yzh119"},"buildBy":[{"avatar_url":"https://avatars.githubusercontent.com/u/11773619?s=40&v=4","login":"yzh119"},{"avatar_url":"https://avatars.githubusercontent.com/u/2470081?s=40&v=4","login":"abcdabcd987"},{"avatar_url":"https://avatars.githubusercontent.com/u/45167100?s=40&v=4","login":"MasterJH5574"},{"avatar_url":"https://avatars.githubusercontent.com/u/46627482?s=40&v=4","login":"zhyncs"},{"avatar_url":"https://avatars.githubusercontent.com/u/52445717?s=40&v=4","login":"yyihuang"}]},{"full_name":"","description":"CUDA Kernel Benchmarking Library","currentPeriodStars":0,"language":"Cuda","languageColor":"#3A4E3A","stargazers_count":680,"forks_count":0,"owner":{"avatar_url":"https://avatars.githubusercontent.com/u/58744?s=40&v=4","login":"alliepiper"},"buildBy":[{"avatar_url":"https://avatars.githubusercontent.com/u/58744?s=40&v=4","login":"alliepiper"},{"avatar_url":"https://avatars.githubusercontent.com/u/9890394?s=40&v=4","login":"gevtushenko"},{"avatar_url":"https://avatars.githubusercontent.com/u/215756?s=40&v=4","login":"robertmaynard"},{"avatar_url":"https://avatars.githubusercontent.com/u/12716979?s=40&v=4","login":"PointKernel"},{"avatar_url":"https://avatars.githubusercontent.com/u/1538165?s=40&v=4","login":"vyasr"}]},{"full_name":"","description":"LLM training in simple, raw C/CUDA","currentPeriodStars":9,"language":"Cuda","languageColor":"#3A4E3A","stargazers_count":27134,"forks_count":0,"owner":{"avatar_url":"https://avatars.githubusercontent.com/u/241138?s=40&v=4","login":"karpathy"},"buildBy":[{"avatar_url":"https://avatars.githubusercontent.com/u/241138?s=40&v=4","login":"karpathy"},{"avatar_url":"https://avatars.githubusercontent.com/u/7938269?s=40&v=4","login":"ngc92"},{"avatar_url":"https://avatars.githubusercontent.com/u/29271842?s=40&v=4","login":"gordicaleksa"},{"avatar_url":"https://avatars.githubusercontent.com/u/7082233?s=40&v=4","login":"ademeure"},{"avatar_url":"https://avatars.githubusercontent.com/u/55313766?s=40&v=4","login":"rosslwheeler"}]},{"full_name":"","description":"","currentPeriodStars":0,"stargazers_count":0,"forks_count":0,"owner":{},"buildBy":[]},{"full_name":"","description":"[ARCHIVED] Cooperative primitives for CUDA C++. See https://github.com/NVIDIA/cccl","currentPeriodStars":0,"language":"Cuda","languageColor":"#3A4E3A","stargazers_count":1757,"forks_count":0,"owner":{"avatar_url":"https://avatars.githubusercontent.com/u/9890394?s=40&v=4","login":"gevtushenko"},"buildBy":[{"avatar_url":"https://avatars.githubusercontent.com/u/9890394?s=40&v=4","login":"gevtushenko"},{"avatar_url":"https://avatars.githubusercontent.com/u/58744?s=40&v=4","login":"alliepiper"},{"avatar_url":"https://avatars.githubusercontent.com/u/1476032?s=40&v=4","login":"dumerrill"},{"avatar_url":"https://avatars.githubusercontent.com/u/398194?s=40&v=4","login":"brycelelbach"},{"avatar_url":"https://avatars.githubusercontent.com/u/3958403?s=40&v=4","login":"elstehle"}]},{"full_name":"","description":"NCCL Tests","currentPeriodStars":0,"language":"Cuda","languageColor":"#3A4E3A","stargazers_count":1177,"forks_count":0,"owner":{"avatar_url":"https://avatars.githubusercontent.com/u/2538330?s=40&v=4","login":"AddyLaddy"},"buildBy":[{"avatar_url":"https://avatars.githubusercontent.com/u/2538330?s=40&v=4","login":"AddyLaddy"},{"avatar_url":"https://avatars.githubusercontent.com/u/12857445?s=40&v=4","login":"sjeaugey"},{"avatar_url":"https://avatars.githubusercontent.com/u/2293859?s=40&v=4","login":"jbachan"},{"avatar_url":"https://avatars.githubusercontent.com/u/687269?s=40&v=4","login":"lukeyeager"},{"avatar_url":"https://avatars.githubusercontent.com/u/6690627?s=40&v=4","login":"jithinjosepkl"}]},{"full_name":"","description":"This is a series of GPU optimization topics. Here we will introduce how to optimize the CUDA kernel in detail. I will introduce several basic kernel optimizations, including: elementwise, reduce, sgemv, sgemm, etc. The performance of these kernels is basically at or near the theoretical limit.","currentPeriodStars":0,"language":"Cuda","languageColor":"#3A4E3A","stargazers_count":1090,"forks_count":0,"owner":{"avatar_url":"https://avatars.githubusercontent.com/u/85323580?s=40&v=4","login":"Liu-xiandong"},"buildBy":[{"avatar_url":"https://avatars.githubusercontent.com/u/85323580?s=40&v=4","login":"Liu-xiandong"},{"avatar_url":"https://avatars.githubusercontent.com/u/63796752?s=40&v=4","login":"ZhangGe6"}]}]}