{"updateTime":"2022-12-27 02:19:36","data":[{"full_name":"Tony-Tan/CUDA_Freshman","description":"","currentPeriodStars":0,"stargazers_count":0,"forks_count":0,"owner":{},"buildBy":[]},{"full_name":"NVlabs/instant-ngp","description":"Instant neural graphics primitives: lightning fast NeRF and more","currentPeriodStars":401,"language":"Cuda","languageColor":"#3A4E3A","stargazers_count":10492,"forks_count":1244,"owner":{"avatar_url":"https://avatars.githubusercontent.com/u/4923655?s=40&v=4","login":"Tom94"},"buildBy":[{"avatar_url":"https://avatars.githubusercontent.com/u/4923655?s=40&v=4","login":"Tom94"},{"avatar_url":"https://avatars.githubusercontent.com/u/7601391?s=40&v=4","login":"FlorisE"},{"avatar_url":"https://avatars.githubusercontent.com/u/29726242?s=40&v=4","login":"jc211"},{"avatar_url":"https://avatars.githubusercontent.com/u/3280839?s=40&v=4","login":"JamesPerlman"},{"avatar_url":"https://avatars.githubusercontent.com/u/22482773?s=40&v=4","login":"koktavy"}]},{"full_name":"NVIDIA/TransformerEngine","description":"A library for accelerating Transformer models on NVIDIA GPUs, including using 8-bit floating point (FP8) precision on Hopper GPUs, to provide better performance with lower memory utilization in both training and inference.","currentPeriodStars":25,"language":"Cuda","languageColor":"#3A4E3A","stargazers_count":324,"forks_count":29,"owner":{"avatar_url":"https://avatars.githubusercontent.com/u/8398980?s=40&v=4","login":"ptrendx"},"buildBy":[{"avatar_url":"https://avatars.githubusercontent.com/u/8398980?s=40&v=4","login":"ptrendx"},{"avatar_url":"https://avatars.githubusercontent.com/u/36168853?s=40&v=4","login":"ksivaman"},{"avatar_url":"https://avatars.githubusercontent.com/u/4406448?s=40&v=4","login":"timmoon10"},{"avatar_url":"https://avatars.githubusercontent.com/u/883319?s=40&v=4","login":"seanprime7"},{"avatar_url":"https://avatars.githubusercontent.com/u/96238833?s=40&v=4","login":"nzmora-nvidia"}]}]}