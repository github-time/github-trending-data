{"updateTime":"2025-09-27 02:28:52","data":[{"full_name":"","description":"12 Weeks, 24 Lessons, AI for All!","currentPeriodStars":2070,"language":"Jupyter Notebook","languageColor":"#DA5B0B","stargazers_count":42455,"forks_count":0,"owner":{"avatar_url":"https://avatars.githubusercontent.com/u/2511341?s=40&v=4","login":"leestott"},"buildBy":[{"avatar_url":"https://avatars.githubusercontent.com/u/2511341?s=40&v=4","login":"leestott"},{"avatar_url":"https://avatars.githubusercontent.com/u/1450004?s=40&v=4","login":"jlooper"},{"avatar_url":"https://avatars.githubusercontent.com/u/2892110?s=40&v=4","login":"shwars"},{"avatar_url":"https://avatars.githubusercontent.com/u/44121227?s=40&v=4","login":"BethanyJep"},{"avatar_url":"https://avatars.githubusercontent.com/in/29110?s=40&v=4","login":"dependabot"}]},{"full_name":"","description":"Offline speech recognition API for Android, iOS, Raspberry Pi and servers with Python, Java, C# and Node","currentPeriodStars":81,"language":"Jupyter Notebook","languageColor":"#DA5B0B","stargazers_count":13301,"forks_count":0,"owner":{"avatar_url":"https://avatars.githubusercontent.com/u/2886672?s=40&v=4","login":"nshmyrev"},"buildBy":[{"avatar_url":"https://avatars.githubusercontent.com/u/2886672?s=40&v=4","login":"nshmyrev"},{"avatar_url":"https://avatars.githubusercontent.com/u/68805308?s=40&v=4","login":"vadimdddd"},{"avatar_url":"https://avatars.githubusercontent.com/u/32008975?s=40&v=4","login":"nnkalita"},{"avatar_url":"https://avatars.githubusercontent.com/u/1008395?s=40&v=4","login":"lkiesow"},{"avatar_url":"https://avatars.githubusercontent.com/u/1866161?s=40&v=4","login":"nalbion"}]},{"full_name":"","description":"Qwen3-VL is the multimodal large language model series developed by Qwen team, Alibaba Cloud.","currentPeriodStars":663,"language":"Jupyter Notebook","languageColor":"#DA5B0B","stargazers_count":13253,"forks_count":0,"owner":{"avatar_url":"https://avatars.githubusercontent.com/u/43326198?s=40&v=4","login":"ShuaiBai623"},"buildBy":[{"avatar_url":"https://avatars.githubusercontent.com/u/43326198?s=40&v=4","login":"ShuaiBai623"},{"avatar_url":"https://avatars.githubusercontent.com/u/136600500?s=40&v=4","login":"kq-chen"},{"avatar_url":"https://avatars.githubusercontent.com/u/12120155?s=40&v=4","login":"fyabc"},{"avatar_url":"https://avatars.githubusercontent.com/u/13191886?s=40&v=4","login":"jinze1994"},{"avatar_url":"https://avatars.githubusercontent.com/u/22515857?s=40&v=4","login":"LibertFan"}]},{"full_name":"","description":"ğŸ“š ä»é›¶å¼€å§‹çš„å¤§è¯­è¨€æ¨¡å‹åŸç†ä¸å®è·µæ•™ç¨‹","currentPeriodStars":657,"language":"Jupyter Notebook","languageColor":"#DA5B0B","stargazers_count":18334,"forks_count":0,"owner":{"avatar_url":"https://avatars.githubusercontent.com/u/77671993?s=40&v=4","login":"KMnO4-zx"},"buildBy":[{"avatar_url":"https://avatars.githubusercontent.com/u/77671993?s=40&v=4","login":"KMnO4-zx"},{"avatar_url":"https://avatars.githubusercontent.com/u/74288839?s=40&v=4","login":"logan-zou"},{"avatar_url":"https://avatars.githubusercontent.com/u/184225750?s=40&v=4","login":"xinala-781"},{"avatar_url":"https://avatars.githubusercontent.com/u/58305964?s=40&v=4","login":"Zeyi-Lin"},{"avatar_url":"https://avatars.githubusercontent.com/u/147008135?s=40&v=4","login":"MengYue-MK2000"}]},{"full_name":"","description":"Neural building blocks for speaker diarization: speech activity detection, speaker change detection, overlapped speech detection, speaker embedding","currentPeriodStars":54,"language":"Jupyter Notebook","languageColor":"#DA5B0B","stargazers_count":8357,"forks_count":0,"owner":{"avatar_url":"https://avatars.githubusercontent.com/u/1080837?s=40&v=4","login":"hbredin"},"buildBy":[{"avatar_url":"https://avatars.githubusercontent.com/u/1080837?s=40&v=4","login":"hbredin"},{"avatar_url":"https://avatars.githubusercontent.com/u/5440566?s=40&v=4","login":"mogwai"},{"avatar_url":"https://avatars.githubusercontent.com/u/14005967?s=40&v=4","login":"FrenchKrab"},{"avatar_url":"https://avatars.githubusercontent.com/u/5760780?s=40&v=4","login":"juanmc2005"},{"avatar_url":"https://avatars.githubusercontent.com/u/55240756?s=40&v=4","login":"clement-pages"}]},{"full_name":"","description":"é¢å‘å¼€å‘è€…çš„ LLM å…¥é—¨æ•™ç¨‹ï¼Œå´æ©è¾¾å¤§æ¨¡å‹ç³»åˆ—è¯¾ç¨‹ä¸­æ–‡ç‰ˆ","currentPeriodStars":109,"language":"Jupyter Notebook","languageColor":"#DA5B0B","stargazers_count":21185,"forks_count":0,"owner":{"avatar_url":"https://avatars.githubusercontent.com/u/74288839?s=40&v=4","login":"logan-zou"},"buildBy":[{"avatar_url":"https://avatars.githubusercontent.com/u/74288839?s=40&v=4","login":"logan-zou"},{"avatar_url":"https://avatars.githubusercontent.com/u/84648701?s=40&v=4","login":"Beyondzjl"},{"avatar_url":"https://avatars.githubusercontent.com/u/64852985?s=40&v=4","login":"xuhu0115"},{"avatar_url":"https://avatars.githubusercontent.com/u/65588374?s=40&v=4","login":"Weihong-Liu"},{"avatar_url":"https://avatars.githubusercontent.com/u/140427007?s=40&v=4","login":"Aphasia0515"}]},{"full_name":"","description":"Understanding Deep Learning - Simon J.D. Prince","currentPeriodStars":242,"language":"Jupyter Notebook","languageColor":"#DA5B0B","stargazers_count":8164,"forks_count":0,"owner":{"avatar_url":"https://avatars.githubusercontent.com/u/110402648?s=40&v=4","login":"udlbook"},"buildBy":[{"avatar_url":"https://avatars.githubusercontent.com/u/110402648?s=40&v=4","login":"udlbook"},{"avatar_url":"https://avatars.githubusercontent.com/u/50220137?s=40&v=4","login":"tomheaton"},{"avatar_url":"https://avatars.githubusercontent.com/u/38562595?s=40&v=4","login":"pitmonticone"},{"avatar_url":"https://avatars.githubusercontent.com/u/10063907?s=40&v=4","login":"yrahal"},{"avatar_url":"https://avatars.githubusercontent.com/u/111278851?s=40&v=4","login":"aleksandrskoselevs"}]},{"full_name":"","description":"The repository provides code for running inference with the Meta Segment Anything Model 2 (SAM 2), links for downloading the trained model checkpoints, and example notebooks that show how to use the model.","currentPeriodStars":84,"language":"Jupyter Notebook","languageColor":"#DA5B0B","stargazers_count":17042,"forks_count":0,"owner":{"avatar_url":"https://avatars.githubusercontent.com/u/6997335?s=40&v=4","login":"ronghanghu"},"buildBy":[{"avatar_url":"https://avatars.githubusercontent.com/u/6997335?s=40&v=4","login":"ronghanghu"},{"avatar_url":"https://avatars.githubusercontent.com/u/48327001?s=40&v=4","login":"NielsRogge"},{"avatar_url":"https://avatars.githubusercontent.com/u/8292193?s=40&v=4","login":"haithamkhedr"},{"avatar_url":"https://avatars.githubusercontent.com/u/25299377?s=40&v=4","login":"arun477"},{"avatar_url":"https://avatars.githubusercontent.com/u/135471798?s=40&v=4","login":"CharlesCNorton"}]},{"full_name":"","description":"Benchmarking Knowledge Transfer in Lifelong Robot Learning","currentPeriodStars":29,"language":"Jupyter Notebook","languageColor":"#DA5B0B","stargazers_count":912,"forks_count":0,"owner":{"avatar_url":"https://avatars.githubusercontent.com/u/21077484?s=40&v=4","login":"zhuyifengzju"},"buildBy":[{"avatar_url":"https://avatars.githubusercontent.com/u/21077484?s=40&v=4","login":"zhuyifengzju"},{"avatar_url":"https://avatars.githubusercontent.com/u/8097542?s=40&v=4","login":"Cranial-XIX"},{"avatar_url":"https://avatars.githubusercontent.com/u/17945444?s=40&v=4","login":"liuzuxin"},{"avatar_url":"https://avatars.githubusercontent.com/u/48041218?s=40&v=4","login":"HeegerGao"}]},{"full_name":"","description":"Learn Agentic AI using Dapr Agentic Cloud Ascent (DACA) Design Pattern and Agent-Native Cloud Technologies: OpenAI Agents SDK, Memory, MCP, A2A, Knowledge Graphs, Dapr, Rancher Desktop, and Kubernetes.","currentPeriodStars":32,"language":"Jupyter Notebook","languageColor":"#DA5B0B","stargazers_count":3664,"forks_count":0,"owner":{"avatar_url":"https://avatars.githubusercontent.com/u/545458?s=40&v=4","login":"ziaukhan"},"buildBy":[{"avatar_url":"https://avatars.githubusercontent.com/u/545458?s=40&v=4","login":"ziaukhan"},{"avatar_url":"https://avatars.githubusercontent.com/u/28400845?s=40&v=4","login":"mjunaidca"},{"avatar_url":"https://avatars.githubusercontent.com/u/10209765?s=40&v=4","login":"EnggQasim"},{"avatar_url":"https://avatars.githubusercontent.com/u/112770629?s=40&v=4","login":"Wania-Kazmi"},{"avatar_url":"https://avatars.githubusercontent.com/u/31050254?s=40&v=4","login":"Ameen-Alam"}]},{"full_name":"","description":"","currentPeriodStars":0,"stargazers_count":0,"forks_count":0,"owner":{},"buildBy":[]},{"full_name":"","description":"Jupyter notebooks for the code samples of the book \"Deep Learning with Python\"","currentPeriodStars":51,"language":"Jupyter Notebook","languageColor":"#DA5B0B","stargazers_count":19558,"forks_count":0,"owner":{"avatar_url":"https://avatars.githubusercontent.com/u/710255?s=40&v=4","login":"fchollet"},"buildBy":[{"avatar_url":"https://avatars.githubusercontent.com/u/710255?s=40&v=4","login":"fchollet"},{"avatar_url":"https://avatars.githubusercontent.com/u/1389937?s=40&v=4","login":"mattdangerw"},{"avatar_url":"https://avatars.githubusercontent.com/u/54595311?s=40&v=4","login":"var-nan"},{"avatar_url":"https://avatars.githubusercontent.com/u/1457728?s=40&v=4","login":"DerekChia"},{"avatar_url":"https://avatars.githubusercontent.com/u/15246198?s=40&v=4","login":"Kuz-man"}]},{"full_name":"","description":"Jupyter notebooks and other resources for Think Python by Allen Downey, published by O'Reilly Media.","currentPeriodStars":50,"language":"Jupyter Notebook","languageColor":"#DA5B0B","stargazers_count":2316,"forks_count":0,"owner":{"avatar_url":"https://avatars.githubusercontent.com/u/1882093?s=40&v=4","login":"AllenDowney"},"buildBy":[{"avatar_url":"https://avatars.githubusercontent.com/u/1882093?s=40&v=4","login":"AllenDowney"},{"avatar_url":"https://avatars.githubusercontent.com/u/3073735?s=40&v=4","login":"wogsland"}]},{"full_name":"","description":"YSDA course in Natural Language Processing","currentPeriodStars":21,"language":"Jupyter Notebook","languageColor":"#DA5B0B","stargazers_count":10307,"forks_count":0,"owner":{"avatar_url":"https://avatars.githubusercontent.com/u/3491902?s=40&v=4","login":"justheuristic"},"buildBy":[{"avatar_url":"https://avatars.githubusercontent.com/u/3491902?s=40&v=4","login":"justheuristic"},{"avatar_url":"https://avatars.githubusercontent.com/u/11651532?s=40&v=4","login":"drt7"},{"avatar_url":"https://avatars.githubusercontent.com/u/16096019?s=40&v=4","login":"lena-voita"},{"avatar_url":"https://avatars.githubusercontent.com/u/5002721?s=40&v=4","login":"kovarsky"},{"avatar_url":"https://avatars.githubusercontent.com/u/24738311?s=40&v=4","login":"poedator"}]},{"full_name":"","description":"ğŸ¦œğŸ”— Build context-aware reasoning applications","currentPeriodStars":521,"language":"Jupyter Notebook","languageColor":"#DA5B0B","stargazers_count":116272,"forks_count":0,"owner":{"avatar_url":"https://avatars.githubusercontent.com/u/22008038?s=40&v=4","login":"baskaryan"},"buildBy":[{"avatar_url":"https://avatars.githubusercontent.com/u/22008038?s=40&v=4","login":"baskaryan"},{"avatar_url":"https://avatars.githubusercontent.com/u/11986836?s=40&v=4","login":"hwchase17"},{"avatar_url":"https://avatars.githubusercontent.com/u/26529506?s=40&v=4","login":"ccurme"},{"avatar_url":"https://avatars.githubusercontent.com/u/3205522?s=40&v=4","login":"eyurtsev"},{"avatar_url":"https://avatars.githubusercontent.com/u/61371264?s=40&v=4","login":"mdrxy"}]},{"full_name":"","description":"A curated list of insanely awesome libraries, packages and resources for Quants (Quantitative Finance)","currentPeriodStars":138,"language":"Jupyter Notebook","languageColor":"#DA5B0B","stargazers_count":22258,"forks_count":0,"owner":{"avatar_url":"https://avatars.githubusercontent.com/u/1583922?s=40&v=4","login":"wilsonfreitas"},"buildBy":[{"avatar_url":"https://avatars.githubusercontent.com/u/1583922?s=40&v=4","login":"wilsonfreitas"},{"avatar_url":"https://avatars.githubusercontent.com/u/5049737?s=40&v=4","login":"femtotrader"},{"avatar_url":"https://avatars.githubusercontent.com/u/47279398?s=40&v=4","login":"bbcho"},{"avatar_url":"https://avatars.githubusercontent.com/u/4340156?s=40&v=4","login":"jaymon0703"},{"avatar_url":"https://avatars.githubusercontent.com/u/13077051?s=40&v=4","login":"mirca"}]},{"full_name":"","description":"Qwen2.5-Omni is an end-to-end multimodal model by Qwen team at Alibaba Cloud, capable of understanding text, audio, vision, video, and performing real-time speech generation.","currentPeriodStars":29,"language":"Jupyter Notebook","languageColor":"#DA5B0B","stargazers_count":3666,"forks_count":0,"owner":{"avatar_url":"https://avatars.githubusercontent.com/u/22543219?s=40&v=4","login":"wangxiongts"},"buildBy":[{"avatar_url":"https://avatars.githubusercontent.com/u/22543219?s=40&v=4","login":"wangxiongts"},{"avatar_url":"https://avatars.githubusercontent.com/u/1701379?s=40&v=4","login":"mahone3297"},{"avatar_url":"https://avatars.githubusercontent.com/u/1741543?s=40&v=4","login":"alu042"},{"avatar_url":"https://avatars.githubusercontent.com/u/4702353?s=40&v=4","login":"guotong1988"},{"avatar_url":"https://avatars.githubusercontent.com/u/6215122?s=40&v=4","login":"savy-91"}]}]}