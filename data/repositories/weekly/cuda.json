{"updateTime":"2022-12-22 02:20:14","data":[{"full_name":"NVIDIA/TransformerEngine","description":"A library for accelerating Transformer models on NVIDIA GPUs, including using 8-bit floating point (FP8) precision on Hopper GPUs, to provide better performance with lower memory utilization in both training and inference.","currentPeriodStars":10,"language":"Cuda","languageColor":"#3A4E3A","stargazers_count":321,"forks_count":28,"owner":{"avatar_url":"https://avatars.githubusercontent.com/u/8398980?s=40&v=4","login":"ptrendx"},"buildBy":[{"avatar_url":"https://avatars.githubusercontent.com/u/8398980?s=40&v=4","login":"ptrendx"},{"avatar_url":"https://avatars.githubusercontent.com/u/36168853?s=40&v=4","login":"ksivaman"},{"avatar_url":"https://avatars.githubusercontent.com/u/4406448?s=40&v=4","login":"timmoon10"},{"avatar_url":"https://avatars.githubusercontent.com/u/883319?s=40&v=4","login":"seanprime7"},{"avatar_url":"https://avatars.githubusercontent.com/u/96238833?s=40&v=4","login":"nzmora-nvidia"}]},{"full_name":"leoxiaobin/deep-high-resolution-net.pytorch","description":"The project is an official implementation of our CVPR2019 paper \"Deep High-Resolution Representation Learning for Human Pose Estimation\"","currentPeriodStars":7,"language":"Cuda","languageColor":"#3A4E3A","stargazers_count":3937,"forks_count":887,"owner":{"avatar_url":"https://avatars.githubusercontent.com/u/5362509?s=40&v=4","login":"leoxiaobin"},"buildBy":[{"avatar_url":"https://avatars.githubusercontent.com/u/5362509?s=40&v=4","login":"leoxiaobin"},{"avatar_url":"https://avatars.githubusercontent.com/u/17076122?s=40&v=4","login":"welleast"},{"avatar_url":"https://avatars.githubusercontent.com/u/44516327?s=40&v=4","login":"CrystalSixone"},{"avatar_url":"https://avatars.githubusercontent.com/u/1326899?s=40&v=4","login":"alex9311"},{"avatar_url":"https://avatars.githubusercontent.com/u/2300329?s=40&v=4","login":"gachiemchiep"}]}]}